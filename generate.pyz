#!/usr/bin/env python3

#  Unicorn - Embeddable Unicode Library
#  Copyright (c) 2021-2025 Railgun Labs, LLC - All Rights Reserved
#
#  All information contained herein is and remains the property of Railgun Labs.
#  The intellectual and technical concepts contained herein are proprietary to
#  Railgun Labs and may be covered by U.S. and Foreign Patents, patents in process,
#  and are protected by trade secret or copyright law.
#
#  This Python script is an amalgamation of multiple scripts. It is intended for
#  consumption, not development. As part of the amalgamation process the code
#  has been obfuscated. This includes the removal of comments and type hints.
#  You may not reverse engineer, decompile, disassemble, or otherwise attempt
#  to derive the source code or underlying structure of this script.
#
#  The original source code can be obtained by purchasing a license
#  from Railgun Labs at https://RailgunLabs.com/unicorn/license.

b='16.0.0'
from enum import IntEnum
from dataclasses import dataclass
class eW(IntEnum):
 bl=32
 ek=16
 eJ=8
 def c(self):
  return int(self/8)
@dataclass
class ff:
 def __init__(self,source='',header='',fe='',size=0):
  self.source=source
  self.header=header
  self.fe=fe
  self.size=size
from typing import List,Dict,Any,Optional
from dataclasses import dataclass
eS=int
a=1114112
dE=128
@dataclass
class dF:
 name:str
 dQ:eS
 d:eS
 def __hash__(self):
  return hash(self.name)
 def __eq__(self,dR):
  if isinstance(dR,dF):
   return self.name==dR.name
  return False
def cI(e):
 return [int(fc,16) for fc in e.split()]
class ea:
 def __init__(self,f):
  self.eC=[0]*f
 def __hash__(self):
  bm=5381
  for value in self.eC:
   bm^=hash(value)
  return bm
 def __eq__(self,dR):
  if isinstance(dR,ea):
   if len(self.eC)!=len(dR.eC):
    return False
   for dS in range(len(self.eC)):
    if self.eC[dS]!=dR.eC[dS]:
     return False
   return True
  return False
class bn:
 def __init__(self,index,name,dT,eb):
  self.id=index
  self.g=name
  self.dT=dT
  self.eb=eb
class fd:
 def __init__(self):
  self.characters={}
  self.eK={}
  self.dm=None
 def eV(self,property,dT,eb):
  if property not in self.eK:
   self.eK[property]=bn(len(self.eK),property,dT,eb)
  return self.eK[property].id
 def get(self,property):
  return self.eK[property].id
 def h(self):
  self.dm=ea(len(self.eK))
  for eD in self.eK.values():
   self.dm.eC[eD.id]=eD.dT
 def bo(self,fc):
  if fc in self.characters:
   return self.characters[fc]
  else:
   ep=ea(len(self.eK))
   for eD in self.eK.values():
    ep.eC[eD.id]=eD.dT
   self.characters[fc]=ep
   return ep
 def set(self,fc,property,value):
  self.bo(fc).eC[property]=value
 def ec(self,fc,property,value):
  self.bo(fc).eC[property]|=value
def cM(fj,struct,function):
 if len(fj.eK)==0:
  return ff()
 fj.h()
 el={}
 assert fj.dm is not None
 el[fj.dm]=0
 dn=0
 for eT,ew in fj.characters.items():
  if ew not in el:
   el[ew]=len(el)
  if el[ew]!=0:
   dn=max(dn,eT)
 cJ=[]
 cK=[]
 cL=[]
 eK=sorted(fj.eK.values(),key=lambda er:er.eb,reverse=True)
 for code in range(dn+1):
  if code%dE!=0:
   continue
  dU=[]
  for bp in range(code,code+dE):
   if bp in fj.characters:
    ew=fj.characters[bp]
    if ew in el:
     dU+=[el[ew]]
     continue
   dU+=[0]
  if dU in cL:
   cJ+=[cL.index(dU)*dE]
  else:
   cJ+=[len(cK)]
   cK+=dU
   cL+=[dU]
 size=0
 source=''
 source+=f'const struct {struct} *{function}(unichar cp)\n'
 source+='{\n'
 source+=f'    static const struct {struct} {function}_unicode_codepoints[] = {{\n'
 for ep in el.keys():
  source+='        {'
  for eD in eK:
   source+='{0}u,'.format(ep.eC[eD.id])
   size+=eD.eb.c()
  source+='},\n'
 source+='    };\n\n'
 source+=f'    static const uint16_t {function}_stage1_table[] = {{'
 for index,value in enumerate(cJ):
  if index%8==0:
   source+='\n'
   source+='        '
  source+='%du, '%value
  size+=2
 source+='\n'
 source+='    };\n\n'
 source+=f'    static const uint16_t {function}_stage2_table[] = {{'
 for index,value in enumerate(cK):
  if index%8==0:
   source+='\n'
   source+='        '
  source+='%du, '%value
  size+=2
 source+='\n'
 source+='    };\n\n'
 source+=f'    const struct {struct} *data = NULL;\n'
 source+='    if (cp > UNICHAR_C({0}))\n'.format(dn)
 source+='    {\n'
 source+='        data = &{0}_unicode_codepoints[0]; // code point out of range\n'.format(function)
 source+='    }\n'
 source+='    else\n'
 source+='    {\n'
 source+='        const uint16_t stage2_offset = {0}_stage1_table[cp >> UNICHAR_C({1})];\n'.format(function,dE.bit_length()-1)
 source+='        const uint16_t codepoint_index = {0}_stage2_table[stage2_offset + (cp & UNICHAR_C({1}))];\n'.format(function,dE-1)
 source+='        data = &{0}_unicode_codepoints[codepoint_index];\n'.format(function)
 source+='    }\n'
 source+='\n'
 source+='    return data;\n'
 source+='}\n\n'
 i={eW.eJ:'uint8_t',eW.ek:'uint16_t',eW.bl:'uint32_t'}
 header=''
 header+=f'struct {struct} {{\n'
 for eD in eK:
  header+=f'     {i[eD.eb]} {eD.g};\n'
 header+='};\n'
 header+=f'const struct {struct} *{function}(unichar cp);\n'
 return ff(source,header,'',size)
from typing import List,Set,Dict,Any,Optional,Tuple,cast
import sys
import enum
import json
import zipfile
import fnmatch
class dG(enum.Enum):
 bq=enum.auto()
 cN=enum.auto()
class es(enum.Enum):
 em=enum.auto()
 j=enum.auto()
class ed(enum.IntFlag):
 eE=enum.auto()
 cO=enum.auto()
 cP=enum.auto()
class ee(enum.IntFlag):
 eE=enum.auto()
 br=enum.auto()
 bs=enum.auto()
 bt=enum.auto()
class dH(enum.IntFlag):
 eE=enum.auto()
 bu=enum.auto()
 bv=enum.auto()
class ef(enum.IntFlag):
 eE=enum.auto()
 bw=enum.auto()
 bx=enum.auto()
 by=enum.auto()
class eg(enum.IntFlag):
 eE=enum.auto()
 bz=enum.auto()
 bA=enum.auto()
 bB=enum.auto()
class eZ(enum.IntFlag):
 eE=enum.auto()
 bC=enum.auto()
 bD=enum.auto()
 bE=enum.auto()
 bF=enum.auto()
 bG=enum.auto()
 bH=enum.auto()
 bI=enum.auto()
 bJ=enum.auto()
 bK=enum.auto()
 bL=enum.auto()
 bM=enum.auto()
 bN=enum.auto()
 bO=enum.auto()
 bP=enum.auto()
 bQ=enum.auto()
 bR=enum.auto()
 bS=enum.auto()
 bT=enum.auto()
 bU=enum.auto()
 bV=enum.auto()
class k:
 def __init__(self):
  self.normalization=ed.eE
  self.dV=ee.eE
  self.do=dH.eE
  self.segmentation=eg.eE
  self.compression=False
  self.collation=False
  self.bW=False
class fb:
 def __init__(self):
  self.endian=dG.cN if sys.byteorder=='big' else dG.bq
  self.bX=True
  self.cQ='uint32_t'
  self.optimize=es.em
  self.bY=32
  self.dp=[]
  self.dW=ef.eE
  self.algorithms=k()
  self.eX=eZ.eE
 def l(self,fc):
  blocks=self.dp
  cR=0
  cS=len(blocks)-1
  while cR<=cS:
   dq=(cS+cR)//2
   if fc<blocks[dq].dQ:
    cS=dq-1
   elif fc>blocks[dq].d:
    cR=dq+1
   else:
    return False
  return True
 def m(self):
  if self.cQ.find('64')>0:
   return 8
  return 4
def q(p):
 dI=[]
 for o in p.split('.'):
  dI.append(int(o))
 while len(dI)<2:
  dI.append(0)
 return (dI[0],dI[1])
def eR(bZ,ca):
 bZ.lower().lstrip('is').replace('_','').replace(' ','')
 ca.lower().rstrip('is').replace('_','').replace(' ','')
 return bZ==ca
def t(fi,algorithms):
 if not isinstance(algorithms,Dict):
  print("error: expected object for 'algorithms'")
  sys.exit(1)
 for key,value in algorithms.items():
  if key=='normalization':
   if not isinstance(value,List):
    print("error: expected string list for 'normalization'")
    sys.exit(1)
   for ex in value:
    if not isinstance(ex,str):
     print("error: expected string list for 'normalization'")
     sys.exit(1)
    if ex.lower()=='nfc':
     fi.algorithms.normalization|=ed.cO
    elif ex.lower()=='nfd':
     fi.algorithms.normalization|=ed.cP
    else:
     print('warning: ignoring unknown normalization form: {0}'.format(ex))
  elif key=='normalizationQuickCheck':
   if not isinstance(value,bool):
    print("error: expected boolean value for 'normalizationQuickCheck'")
    sys.exit(1)
   fi.algorithms.bW=value
  elif key=='caseConversion':
   if not isinstance(value,List):
    print("error: expected string list for 'caseConversion'")
    sys.exit(1)
   for ey in value:
    if not isinstance(ey,str):
     print("error: expected string list for 'caseConversion'")
     sys.exit(1)
    if ey.lower()=='lower':
     fi.algorithms.dV|=ee.br
    elif ey.lower()=='upper':
     fi.algorithms.dV|=ee.bs
    elif ey.lower()=='title':
     fi.algorithms.dV|=ee.bt
    else:
     print('warning: ignoring unknown case conversion target: {0}'.format(ey))
  elif key=='caseFolding':
   if not isinstance(value,List):
    print("error: expected string list for 'caseFolding'")
    sys.exit(1)
   for ey in value:
    if not isinstance(ey,str):
     print("error: expected string list for 'caseFolding'")
     sys.exit(1)
    if ey.lower()=='default':
     fi.algorithms.do|=dH.bu
    elif ey.lower()=='canonical':
     fi.algorithms.do|=dH.bv
    else:
     print('warning: ignoring unknown case fold target: {0}'.format(ey))
  elif key=='segmentation':
   if not isinstance(value,List):
    print("error: expected string list for 'segmentation'")
    sys.exit(1)
   for ex in value:
    if not isinstance(ex,str):
     print("error: expected string list for 'segmentation'")
     sys.exit(1)
    if ex.lower()=='grapheme':
     fi.algorithms.segmentation|=eg.bz
    elif ex.lower()=='word':
     fi.algorithms.segmentation|=eg.bA
    elif ex.lower()=='sentence':
     fi.algorithms.segmentation|=eg.bB
    else:
     print('warning: ignoring unknown segmentation form: {0}'.format(ex))
  elif key=='compression':
   if not isinstance(value,bool):
    print("error: expected boolean value for 'compression'")
    sys.exit(1)
   fi.algorithms.compression=value
  elif key=='collation':
   if not isinstance(value,bool):
    print("error: expected boolean value for 'collation'")
    sys.exit(1)
   fi.algorithms.collation=value
  else:
   print('warning: ignoring unknown algorithm: {0}'.format(key))
def parse(filename,fg):
 fi=fb()
 if filename is None:
  return fi
 try:
  cb=open(filename,'r',encoding='utf-8')
 except FileNotFoundError:
  print('error: file not found: {0}'.format(filename))
  sys.exit(1)
 fh=json.load(cb)
 cb.close()
 if 'version' not in fh:
  print('error: missing version: {0}'.format(filename))
  sys.exit(1)
 version=q(fh['version'])
 if version[0]<1:
  print('error: illegal version: {0}'.format(version))
  sys.exit(1)
 cc=set()
 file=fg.open('blocks.json')
 blocks=[dF(*block) for block in json.loads(file.read())['blocks']]
 file.close()
 for key,value in fh.items():
  if key=='version':
   continue
  elif key=='endian':
   if not isinstance(value,str):
    print("error: expected string value for 'endian'")
    sys.exit(1)
   if value.lower()=='little':
    fi.endian=dG.bq
   elif value.lower()=='big':
    fi.endian=dG.cN
   elif value.lower()=='native':
    pass
   else:
    print("warning: expected 'little' or 'big' for 'endian'")
  elif key=='optimizeFor':
   if not isinstance(value,str):
    print("error: expected string value for 'optimizeFor'")
    sys.exit(1)
   if value.lower()=='speed':
    fi.optimize=es.em
   elif value.lower()=='size':
    fi.optimize=es.j
   else:
    print("warning: expected 'speed' or 'size' for 'optimizeFor'")
  elif key=='hasStandardAllocators':
   if not isinstance(value,bool):
    print("error: expected boolean value for 'hasStandardAllocators'")
    sys.exit(1)
   fi.bX=value
  elif key=='characterStorage':
   if not isinstance(value,str) or len(value.strip())==0:
    print("error: expected string value for 'characterStorage'")
    sys.exit(1)
   fi.cQ=value
  elif key=='stackBufferSize':
   if not isinstance(value,int) or value<1:
    print("error: expected a positive integer value for 'stackBufferSize'")
    sys.exit(1)
   cT=4
   if value<cT:
    print(f"warning: rounding up 'stackBufferSize' from {value} to {cT} (the minimum)")
    value=cT
   fi.bY=value
  elif key=='excludeCharacterBlocks':
   if not isinstance(value,List):
    print("error: expected string list for 'excludeCharacterBlocks'")
    sys.exit(1)
   for cd in value:
    if not isinstance(cd,str):
     print("error: expected string list for 'excludeCharacterBlocks'")
     sys.exit(1)
    for block in blocks:
     if fnmatch.fnmatch(block.name,cd):
      cc.add(block)
  elif key=='encodingForms':
   if not isinstance(value,List):
    print("error: expected string list for 'encodingForms'")
    sys.exit(1)
   for encoding in value:
    if not isinstance(encoding,str):
     print("error: expected string list for 'encodingForms'")
     sys.exit(1)
    if encoding.lower()=='utf-8':
     fi.dW|=ef.bw
    elif encoding.lower()=='utf-16':
     fi.dW|=ef.bx
    elif encoding.lower()=='utf-32':
     fi.dW|=ef.by
    else:
     print("warning: expected 'utf-8' or 'utf-16' or 'utf-32' for 'encodingForms'")
  elif key=='characterProperties':
   if not isinstance(value,List):
    print("error: expected string list for 'characterProperties'")
    sys.exit(1)
   for eU in value:
    if not isinstance(eU,str):
     print("error: expected string list for 'characterProperties'")
     sys.exit(1)
    if eR(eU,'Alphabetic'):
     fi.eX|=eZ.bC
    elif eR(eU,'Canonical_Combining_Class'):
     fi.eX|=eZ.bD
    elif eR(eU,'Dash'):
     fi.eX|=eZ.bE
    elif eR(eU,'Diacritic'):
     fi.eX|=eZ.bF
    elif eR(eU,'Extender'):
     fi.eX|=eZ.bG
    elif eR(eU,'General_Category'):
     fi.eX|=eZ.bH
    elif eR(eU,'Hex_Digit'):
     fi.eX|=eZ.bI
    elif eR(eU,'Ideographic'):
     fi.eX|=eZ.bJ
    elif eR(eU,'Lowercase'):
     fi.eX|=eZ.bK
    elif eR(eU,'Math'):
     fi.eX|=eZ.bL
    elif eR(eU,'Noncharacter_Code_Point'):
     fi.eX|=eZ.bM
    elif eR(eU,'Numeric_Value'):
     fi.eX|=eZ.bN
    elif eR(eU,'Quotation_Mark'):
     fi.eX|=eZ.bO
    elif eR(eU,'Simple_Lowercase_Mapping'):
     fi.eX|=eZ.bP
    elif eR(eU,'Simple_Uppercase_Mapping'):
     fi.eX|=eZ.bQ
    elif eR(eU,'Simple_Titlecase_Mapping'):
     fi.eX|=eZ.bR
    elif eR(eU,'Terminal_Punctuation'):
     fi.eX|=eZ.bS
    elif eR(eU,'Unified_Ideograph'):
     fi.eX|=eZ.bT
    elif eR(eU,'Uppercase'):
     fi.eX|=eZ.bU
    elif eR(eU,'White_Space'):
     fi.eX|=eZ.bV
    else:
     print('warning: ignoring unknown character property: {0}'.format(eU))
  elif key=='algorithms':
   t(fi,value)
  else:
   print('warning: ignoring unknown feature: {0}'.format(key))
 for block in cc:
  fi.dp.append(block)
 fi.dp=sorted(fi.dp,key=lambda er:er.dQ)
 return fi
from typing import Set,Tuple,Type
import zipfile
class Feature(object):
 def __init__(self):
  self.dr=0
 def eQ(self,fj):
  pass
 def eY(self,fg,fj,fi):
  return ff()
 def cU(self,fg,fj,fi):
  return ff()
 @staticmethod
 def eP():
  return set()
dX=('flags',0,eW.eJ)
ce=1
u=2
v=4
cf=8
cg=16
from typing import Dict,Set,Type
import zipfile
import json
x=1
y=2
z=4
B=8
C=16
D=32
E=64
F=128
G=256
H=512
J=1024
K=2048
N=4096
O=8192
class P(Feature):
 def eQ(self,fj):
  fj.eV('binary_properties',0,eW.ek)
 def eY(self,fg,fj,fi):
  fe='#define UNICORN_FEATURE_BINARY_PROPERTIES\n'
  return ff(fe=fe)
class eM(Feature):
 @staticmethod
 def eP():
  return set([P])
 def eL(self,fg,fj,Q,R,V):
  file=fg.open('binary_properties.json')
  fh=json.loads(file.read())
  mappings=fh[Q]
  file.close()
  binary_properties=fj.get('binary_properties')
  for fc in mappings.keys():
   fj.ec(int(fc,16),binary_properties,R)
  header='#define {0}\n'.format(V)
  return ff(fe=header)
class W(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isNoncharacterCodePoint',x,'UNICORN_FEATURE_NONCHARACTER_CODE_POINT')
class Y(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isAlphabetic',y,'UNICORN_FEATURE_ALPHABETIC')
class Z(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isLowercase',z,'UNICORN_FEATURE_LOWERCASE')
class aa(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isUppercase',B,'UNICORN_FEATURE_UPPERCASE')
class ab(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isHexDigit',C,'UNICORN_FEATURE_HEX_DIGIT')
class ac(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isWhiteSpace',D,'UNICORN_FEATURE_WHITE_SPACE')
class ad(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isMath',E,'UNICORN_FEATURE_MATH')
class ae(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isDash',F,'UNICORN_FEATURE_DASH')
class af(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isDiacritic',G,'UNICORN_FEATURE_DIACRITIC')
class ag(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isExtender',H,'UNICORN_FEATURE_EXTENDER')
class ah(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isIdeographic',J,'UNICORN_FEATURE_IDEOGRAPHIC')
class ai(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isQuotationMark',K,'UNICORN_FEATURE_QUOTATION_MARK')
class aj(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isUnifiedIdeograph',N,'UNICORN_FEATURE_UNIFIED_IDEOGRAPH')
class ak(eM):
 def eY(self,fg,fj,fi):
  return self.eL(fg,fj,'isTerminalPunctuation',O,'UNICORN_FEATURE_TERMINAL_PUNCTUATION')
from typing import Dict
import zipfile
import json
class am(Feature):
 def eQ(self,fj):
  fj.eV('general_category',29,eW.eJ)
 def eY(self,fg,fj,fi):
  file=fg.open('gc.json')
  fh=json.loads(file.read())
  characters=fh['characters']
  file.close()
  al=fj.get('general_category')
  for fc,value in characters.items():
   fj.set(int(fc,16),al,value)
  fe='#define UNICORN_FEATURE_GC\n'
  return ff(fe=fe)
from typing import Dict
import zipfile
import json
class ao(Feature):
 def eQ(self,fj):
  fj.eV('numeric_value_offset',0,eW.eJ)
 def eY(self,fg,fj,fi):
  file=fg.open('numeric_values.json')
  fh=json.loads(file.read())
  mappings=fh['mappings']
  source=fh['source']
  header=fh['header']
  size=fh['size']
  file.close()
  an=fj.get('numeric_value_offset')
  for fc,index in mappings.items():
   fj.set(int(fc,16),an,index)
  fe='#define UNICORN_FEATURE_NUMERIC_VALUE\n'
  return ff(source,header,fe,size)
from typing import List,Dict,Set,Type
import zipfile
import json
et={}
eu=[0]
eH=fd()
dJ=32768
ez=8191
class ci(Feature):
 @staticmethod
 def eP():
  return set([cV])
 def eQ(self,_):
  (eH.eV('simple_lowercase_mapping',ez,eW.ek),)
 def eY(self,fg,_,eh):
  file=fg.open('simple_case_mappings.json')
  fh=json.loads(file.read())
  ap=fh['lowercase']
  file.close()
  ch=eH.get('simple_lowercase_mapping')
  for fc,value in ap.items():
   eT=int(fc,16)
   dK=int(value,16)
   en=eT-dK
   if abs(en)>ez:
    if dK not in et:
     et[dK]=len(eu)
     eu.append(dK)
    eH.set(eT,ch,dJ|et[dK])
   else:
    eH.set(eT,ch,en+ez)
  fe='#define UNICORN_FEATURE_SIMPLE_LOWERCASE_MAPPINGS\n'
  return ff(fe=fe)
class ck(Feature):
 @staticmethod
 def eP():
  return set([cV])
 def eQ(self,_):
  (eH.eV('simple_uppercase_mapping',ez,eW.ek),)
 def eY(self,fg,_,eh):
  file=fg.open('simple_case_mappings.json')
  fh=json.loads(file.read())
  aq=fh['uppercase']
  file.close()
  cj=eH.get('simple_uppercase_mapping')
  for fc,value in aq.items():
   eT=int(fc,16)
   dL=int(value,16)
   en=eT-dL
   if abs(en)>ez:
    if dL not in et:
     et[dL]=len(eu)
     eu.append(dL)
    eH.set(eT,cj,dJ|et[dL])
   else:
    eH.set(eT,cj,en+ez)
  fe='#define UNICORN_FEATURE_SIMPLE_UPPERCASE_MAPPINGS\n'
  return ff(fe=fe)
class cm(Feature):
 @staticmethod
 def eP():
  return set([cV])
 def eQ(self,_):
  (eH.eV('simple_titlecase_mapping',ez,eW.ek),)
 def eY(self,fg,_,eh):
  file=fg.open('simple_case_mappings.json')
  fh=json.loads(file.read())
  ar=fh['titlecase']
  file.close()
  cl=eH.get('simple_titlecase_mapping')
  for fc,value in ar.items():
   eT=int(fc,16)
   dM=int(value,16)
   en=eT-dM
   if abs(en)>ez:
    if dM not in et:
     et[dM]=len(eu)
     eu.append(dM)
    eH.set(eT,cl,dJ|et[dM])
   else:
    eH.set(eT,cl,en+ez)
  fe='#define UNICORN_FEATURE_SIMPLE_TITLECASE_MAPPINGS\n'
  return ff(fe=fe)
class cV(Feature):
 def cU(self,fg,eh,ds):
  size=0
  source='const unichar unicorn_simple_case_mappings[] = {'
  for index,eT in enumerate(eu):
   if index%4==0:
    source+='\n    '
   source+='UNICHAR_C(0x{:04X}), '.format(eT)
   size+=4
  source+='};\n\n'
  header=f'#define CASING_IN_TABLE(C) (((C) & (uint16_t){dJ}) == (uint16_t){dJ})\n'
  header+='#define GET_CASED_VALUE(C) ((C) & (uint16_t)0x3FFF)\n'
  header+=f'#define CASING_DIFF {ez}\n'
  header+='extern const unichar unicorn_simple_case_mappings[{0}];\n'.format(len(eu))
  assert len(eu)<65535,'cannot use unsigned 16-bit integer anymore'
  fh=cM(eH,'CharCaseData','unicorn_get_character_casing')
  source+=fh.source
  header+=fh.header
  size+=fh.size
  return ff(source,header,'',size)
from typing import Dict,Set,Type
import zipfile
import json
class cn(Feature):
 def eQ(self,fj):
  fj.eV(*dX)
 def eY(self,fg,fj,fi):
  file=fg.open('special_case_mappings.json')
  fh=json.loads(file.read())
  source=fh['source']
  header=fh['header']
  size=fh['size']
  mappings=fh['characterFlags']
  file.close()
  dY=fj.get(dX[0])
  header+='#define IS_CASED {0}u\n'.format(u)
  header+='#define IS_CASE_IGNORABLE {0}u\n'.format(v)
  for fc,flags in mappings.items():
   fj.ec(int(fc,16),dY,flags)
  return ff(source=source,header=header,size=size)
class co(Feature):
 @staticmethod
 def eP():
  return set([ci,cn])
 def eY(self,_,eh,ds):
  fe='#define UNICORN_FEATURE_LOWERCASE_CONVERT\n'
  return ff(fe=fe)
class at(Feature):
 @staticmethod
 def eP():
  return set([ck,cn])
 def eY(self,_,eh,ds):
  fe='#define UNICORN_FEATURE_UPPERCASE_CONVERT\n'
  return ff(fe=fe)
class au(Feature):
 @staticmethod
 def eP():
  return set([cm,cp,dN,co])
 def eY(self,_,eh,ds):
  fe='#define UNICORN_FEATURE_TITLECASE_CONVERT\n'
  return ff(fe=fe)
from typing import List,Set,Type,Tuple
import zipfile
import json
class cq(Feature):
 def eQ(self,fj):
  fj.eV('full_casefold_mapping_offset',0,eW.ek)
  fj.eV(*dX)
 def eY(self,fg,fj,fi):
  file=fg.open('casefold.json')
  fh=json.loads(file.read())
  source=fh['source']
  header=fh['header']
  size=fh['size']
  av=fh['hasGreek']
  aw=fh['caseFoldings']
  ax=fh['changesWhenCasefolded']
  file.close()
  dY=fj.get(dX[0])
  ay=fj.get('full_casefold_mapping_offset')
  for fc in av:
   fj.ec(fc,dY,cf)
  for fc in ax:
   fj.ec(fc,dY,cg)
  for fc,offset in aw:
   fj.set(fc,ay,offset)
  header+='#define UNICORN_CHAR_NEEDS_NORMALIZATION {0}u\n'.format(cf)
  header+='#define UNICORN_CHAR_CHANGES_WHEN_CASEFOLDED {0}u\n'.format(cg)
  fe='#define UNICORN_FEATURE_CASEFOLD_DEFAULT\n'
  return ff(source,header,fe,size)
class az(Feature):
 @staticmethod
 def eP():
  return set([cq,dt])
 def eY(self,fg,fj,fi):
  fe='#define UNICORN_FEATURE_CASEFOLD_CANONICAL\n'
  return ff(fe=fe)
from typing import Dict
import zipfile
import json
class dN(Feature):
 def eQ(self,fj):
  fj.eV('canonical_combining_class',0,eW.eJ)
 def eY(self,fg,fj,fi):
  file=fg.open('ccc.json')
  mappings=json.loads(file.read())['ccc']
  file.close()
  aA=fj.get('canonical_combining_class')
  for fc,ccc in mappings.items():
   fj.set(int(fc,16),aA,ccc)
  fe='#define UNICORN_FEATURE_CCC\n'
  return ff(fe=fe)
from typing import Dict,Set,Type
import zipfile
import json
class aC(Feature):
 @staticmethod
 def eP():
  return set([dN])
 def eQ(self,fj):
  fj.eV('quick_check_flags',0,eW.eJ)
 def eY(self,fg,fj,_):
  file=fg.open('quickcheck.json')
  fh=json.loads(file.read())
  aB=fh['quickCheckNFC']
  file.close()
  cW=fj.get('quick_check_flags')
  for fc,cX in aB.items():
   fj.ec(int(fc,16),cW,cX<<4)
  fe='#define UNICORN_FEATURE_NFC_QUICK_CHECK\n'
  return ff(fe=fe)
class aE(Feature):
 @staticmethod
 def eP():
  return set([dN])
 def eQ(self,fj):
  fj.eV('quick_check_flags',0,eW.eJ)
 def eY(self,fg,fj,_):
  file=fg.open('quickcheck.json')
  fh=json.loads(file.read())
  aD=fh['quickCheckNFD']
  file.close()
  cW=fj.get('quick_check_flags')
  for fc,cX in aD.items():
   fj.ec(int(fc,16),cW,cX)
  fe='#define UNICORN_FEATURE_NFD_QUICK_CHECK\n'
  return ff(fe=fe)
from typing import List,Dict,Set,Type
import zipfile
import json
class dt(Feature):
 @staticmethod
 def eP():
  return set([dN,cr])
 def eQ(self,fj):
  fj.eV('canonical_decomposition_mapping_offset',0,eW.ek)
 def eY(self,fg,fj,fi):
  aF=fj.get('canonical_decomposition_mapping_offset')
  file=fg.open('normalize.json')
  mappings=json.loads(file.read())['decompositions']
  file.close()
  cY={}
  eF=[0]
  decompositions={}
  for key,value in mappings.items():
   fc=int(key,16)
   mapping=cI(value)
   decompositions[fc]=mapping
   cZ=''.join([chr(aG) for aG in mapping])
   if cZ in cY:
    canonical_decomposition_mapping_offset=cY[cZ]
   else:
    canonical_decomposition_mapping_offset=len(eF)
    cY[cZ]=len(eF)
    if fi.optimize==es.em:
     eF.append(len(mapping))
     eF+=mapping
    else:
     ei=[]
     for ep in mapping:
      ei+=[int(da) for da in chr(ep).encode('utf8')]
     assert len(ei)<255
     eF.append(len(ei))
     eF+=ei
   fj.set(fc,aF,canonical_decomposition_mapping_offset)
  size=len(eF)
  if fi.optimize==es.em:
   size*=fi.m()
  du=0
  dv=0
  for fc,cs in decompositions.items():
   for fc in cs:
    dw=[fc]
    dO=0
    while dO<len(dw):
     ct=dw[dO]
     if ct not in decompositions:
      dO+=1
      continue
     dw[dO:dO+1]=decompositions[ct]
    ei=[]
    for ep in cs:
     ei+=[int(da) for da in chr(ep).encode('utf8')]
    dv=max(dv,len(dw))
    if fi.optimize==es.em:
     du=dv
    else:
     du=max(du,len(ei))
  if fi.optimize==es.em:
   source='const unichar uni_canonical_decomp_mappings[] = {'
   for index,eT in enumerate(eF):
    if index%6==0:
     source+='\n'
     source+='    '
    source+='UNICHAR_C(0x{:02X}), '.format(eT)
   source+='\n'
   source+='};\n\n'
  else:
   source='const uint8_t uni_canonical_decomp_mappings[] = {'
   for index,eT in enumerate(eF):
    if index%8==0:
     source+='\n'
     source+='    '
    source+='0x{:02X}u, '.format(eT)
   source+='\n'
   source+='};\n\n'
  if fi.optimize==es.em:
   header='extern const unichar uni_canonical_decomp_mappings[{0}];\n'.format(len(eF))
  else:
   header='extern const uint8_t uni_canonical_decomp_mappings[{0}];\n'.format(len(eF))
  header+='#define LONGEST_UNICHAR_DECOMPOSITION {0}\n'.format(dv+1)
  header+='#define LONGEST_RAW_DECOMPOSITION {0}\n'.format(du+1)
  fe='#define UNICORN_FEATURE_NFD\n'
  return ff(source,header,fe,size)
from typing import List,Set,Tuple,Type
import zipfile
import json
class aL(Feature):
 @staticmethod
 def eP():
  return set([dt])
 def eQ(self,fj):
  fj.eV('canonical_composition_mapping_offset',0,eW.ek)
  fj.eV('canonical_composition_mapping_count',0,eW.eJ)
  fj.eV(*dX)
 def eY(self,fg,fj,fi):
  file=fg.open('normalize.json')
  fh=json.loads(file.read())
  mappings=fh['compositionMappings']
  cu=fh['compositionPairs']
  file.close()
  aH=fj.get('canonical_composition_mapping_offset')
  aI=fj.get('canonical_composition_mapping_count')
  dY=fj.get(dX[0])
  for db in mappings:
   dQ=db[0]
   offset=db[1]
   count=db[2]
   fj.set(dQ,aH,offset)
   fj.set(dQ,aI,count)
   fj.ec(dQ,dY,ce)
  size=0
  source='const struct CanonicalCompositionPair uni_canonical_comp_pairs[] = {\n'
  for cv in cu:
   aJ=cv[0]
   aK=cv[1]
   source+='    {{ UNICHAR_C(0x{:04X}), UNICHAR_C(0x{:04X}) }},\n'.format(aJ,aK)
   size+=8
  source+='};\n\n'
  header='struct CanonicalCompositionPair\n'
  header+='{\n'
  header+='    unichar codepoint;\n'
  header+='    unichar composed_codepoint;\n'
  header+='};\n'
  header+='#define CHAR_IS_COMPOSABLE {0}u\n'.format(ce)
  header+='extern const struct CanonicalCompositionPair uni_canonical_comp_pairs[{0}];\n'.format(len(cu))
  header+='\n'
  fe='#define UNICORN_FEATURE_NFC\n'
  return ff(source,header,fe,size)
import zipfile
class aM(Feature):
 def eY(self,fg,fj,fi):
  fe='#define UNICORN_FEATURE_COMPRESSION\n'
  return ff(fe=fe)
from typing import List,Dict,Set,Type
import zipfile
import json
aN=1<<31
dc=aN
cw=1<<30
cx=1<<29
cy=dc
class CollationNode:
 def __init__(self):
  self.ew=0
  self.eG={}
  self.cz=0
  self.dZ=0
  self.cA=0
 def cB(self,fc):
  if fc in self.eG:
   return self.eG[fc]
  eO=CollationNode()
  eO.ew=fc
  self.eG[fc]=eO
  return eO
 def cC(self,eA):
  eG=list(self.eG.values())
  eG=sorted(eG,key=lambda er:er.ew)
  if len(eG)>0:
   self.cz=len(eA)
   for dx in eG:
    dx.cA=len(eA)
    eA.append(dx)
   for dx in eG:
    dx.cC(eA)
 def aO(self):
  eA=[]
  self.cC(eA)
  return eA
class aV(Feature):
 @staticmethod
 def eP():
  return set([dt])
 def eQ(self,fj):
  fj.eV('collation_subtype',0,eW.eJ)
 def eY(self,fg,fj,fi):
  dP=[0]
  dy={}
  def cD(ev):
   nonlocal dP
   nonlocal dy
   dz=[]
   for dS,eo in enumerate(ev):
    if dS<len(ev)-1:
     dz.append(eo|cy)
    else:
     dz.append(eo)
   dd=' '.join([str(eo) for eo in dz])
   if dd in dy:
    return dy[dd]
   else:
    dZ=len(dP)
    for eo in dz:
     dP.append(eo)
    dy[dd]=dZ
    return dZ
  collation=fd()
  de=collation.eV('value',0,eW.bl)
  file=fg.open('allkeys.json')
  fh=json.loads(file.read())
  aP=fh['header']
  mappings=fh['mappings']
  cE=fh['contractionStarters']
  aQ=fh['longestInitialSubstring']
  subtypes=fh['subtypes']
  file.close()
  aR=fj.get('collation_subtype')
  for key,aS in subtypes.items():
   eT=int(key,16)
   fj.set(eT,aR,aS)
  root=CollationNode()
  for string,ev in mappings.items():
   dA=string.split()[0]
   if not fi.l(int(dA,16)):
    continue
   if dA in cE:
    ej=cI(string)
    eO=root
    for fc in ej:
     eO=eO.cB(fc)
    assert eO.dZ==0,'expected contractions to be unqiue'
    eO.dZ=cD(ev)
  eA=root.aO()
  df=0
  for string,ev in mappings.items():
   df=max(len(ev),df)
   ej=cI(string)
   dA=string.split()[0]
   if dA in cE:
    eO=root.cB(ej[0])
    collation.set(ej[0],de,eO.cA|cx)
   else:
    assert len(ej)==1,'expected a single character mapping'
    if len(ev)==1:
     collation.set(ej[0],de,ev[0]|dc)
    else:
     collation.set(ej[0],de,cD(ev)|cw)
  fh=cM(collation,'CollationChararacterData','uni_get_collation_data')
  size=fh.size
  source=fh.source
  source+='const uint32_t unicorn_collation_mappings[] = {\n'
  for dS,eo in enumerate(dP):
   if dS%4==0:
    source+='\n    '
   source+='0x{:08X}u, '.format(eo)
   size+=4
  source+='};\n\n'
  source+='const struct CollationNode unicorn_collation_mappings_trie[] = {\n'
  for eO in eA:
   assert len(eO.eG)<=255
   er=len(eO.eG)<<24|eO.ew
   aT=eO.dZ
   aU=eO.cz
   source+=f'    {{ {er}u, {aT}, {aU} }},\n'
   size+=8
  source+='};\n\n'
  header=fh.header
  header+=aP
  header+='#define UNICORN_MAX_COLLATION {0}\n'.format(df+1)
  header+='#define INLINE_CE_FLAG {0}u\n'.format(dc)
  header+='#define IN_WEIGHT_TABLE_FLAG {0}u\n'.format(cw)
  header+='#define IN_TRIE_FLAG {0}u\n'.format(cx)
  header+='#define CONTINUATION_FLAG {0}u\n'.format(cy)
  header+='#define LONGEST_INITIAL_SUBSTRING {0}\n'.format(aQ)
  header+='#define UNICORN_IN_COLLATION_TABLE(NODE) ((NODE)->collation_mappings_offset > (uint16_t)0)\n'
  header+='#define GET_NODE_CODE_POINT(NODE) ((NODE)->codepoint_and_childcount & 0x1FFFFFu)\n'
  header+='#define GET_NODE_CHILD_COUNT(NODE) ((NODE)->codepoint_and_childcount >> 24u)\n'
  header+='struct CollationNode {\n'
  header+='    uint32_t codepoint_and_childcount;\n'
  header+='    uint16_t collation_mappings_offset;\n'
  header+='    uint16_t child_offset;\n'
  header+='};\n'
  header+='extern const struct CollationNode unicorn_collation_mappings_trie[{0}];\n'.format(len(eA))
  header+='extern const uint32_t unicorn_collation_mappings[{0}];\n'.format(len(dP))
  fe='#define UNICORN_FEATURE_COLLATION\n'
  return ff(source,header,fe,size)
from typing import Set,Type
import zipfile
import json
eB=0
class dg(Feature):
 def cU(self,fg,eh,ds):
  gcb=fg.open('segmentation.json')
  fh=json.loads(gcb.read())
  header=fh['header']
  gcb.close()
  header+='#define MAX_BREAK_STATES {0}\n'.format(eB)
  fe='#define UNICORN_FEATURE_SEGMENTATION\n'
  return ff(header=header,fe=fe)
class aY(Feature):
 @staticmethod
 def eP():
  return set([dg])
 def eQ(self,fj):
  fj.eV('gcb',0,eW.eJ)
  fj.eV('incb',0,eW.eJ)
 def eY(self,fg,fj,_):
  gcb=fg.open('gcb.json')
  fh=json.loads(gcb.read())
  header=fh['header']
  source=fh['source']
  size=fh['size']
  gcb.close()
  global eB
  eB=max(int(fh['states']),eB)
  aW=fj.get('gcb')
  for eN in fh['gcb']:
   fj.set(eN[0],aW,eN[1])
  aX=fj.get('incb')
  for eN in fh['incb']:
   fj.set(eN[0],aX,eN[1])
  fe='#define UNICORN_FEATURE_GCB\n'
  return ff(source,header,fe,size)
class cp(Feature):
 @staticmethod
 def eP():
  return set([dg])
 def eQ(self,fj):
  fj.eV('wb',0,eW.eJ)
  fj.eV('wbx',0,eW.eJ)
 def eY(self,fg,fj,_):
  wb=fg.open('wb.json')
  fh=json.loads(wb.read())
  header=fh['header']
  source=fh['source']
  size=fh['size']
  wb.close()
  global eB
  eB=max(int(fh['states']),eB)
  dh=fj.get('wb')
  for eN in fh['wb']:
   fj.set(eN[0],dh,eN[1])
  aZ=fj.get('wbx')
  for eN in fh['wbx']:
   fj.set(eN[0],aZ,eN[1])
  fe='#define UNICORN_FEATURE_WB\n'
  return ff(source,header,fe,size)
class ba(Feature):
 @staticmethod
 def eP():
  return set([dg])
 def eQ(self,fj):
  fj.eV('sb',0,eW.eJ)
 def eY(self,fg,fj,_):
  wb=fg.open('sb.json')
  fh=json.loads(wb.read())
  header=fh['header']
  source=fh['source']
  size=fh['size']
  wb.close()
  global eB
  eB=max(int(fh['states']),eB)
  dh=fj.get('sb')
  for eN in fh['sb']:
   fj.set(eN[0],dh,eN[1])
  fe='#define UNICORN_FEATURE_SB\n'
  return ff(source,header,fe,size)
import zipfile
class cr(Feature):
 def eY(self,fg,fj,fi):
  fe='#define UNICORN_FEATURE_ENCODING_UTF8\n'
  return ff(fe=fe)
class bb(Feature):
 def eY(self,fg,fj,fi):
  fe='#define UNICORN_FEATURE_ENCODING_UTF16\n'
  return ff(fe=fe)
class bc(Feature):
 def eY(self,fg,fj,fi):
  fe='#define UNICORN_FEATURE_ENCODING_UTF32\n'
  return ff(fe=fe)
from typing import List,Set,Tuple,Type
import sys
import os
import argparse
import zipfile
class di(argparse.Namespace):
 def __init__(self):
  self.config_file='features.json'
  self.output_dir=''
  self.verbose=False
def dj(s):
 return '\n'.join((cF for cF in s.splitlines() if cF.strip()))
def be(args,fg):
 header=''
 source=''
 fe=''
 fa=set()
 fi=parse(args.config_file,fg)
 fe+='#include <stdint.h>\n'
 if fi.endian==dG.cN:
  fe+='#define UNICORN_BIG_ENDIAN\n'
 else:
  fe+='#define UNICORN_LITTLE_ENDIAN\n'
 fe+='#define UNICORN_STACK_BUFFER_SIZE {0}\n'.format(fi.bY)
 fe+='typedef {0} unichar;\n'.format(fi.cQ)
 if fi.bX:
  fe+='#define UNICORN_HAVE_C_MEMORY_ROUTINES\n'
 header+='#include "unicorn.h"\n'
 dk=b.split('.')
 header+='#define UNICODE_VERSION_MAJOR {0}\n'.format(dk[0])
 header+='#define UNICODE_VERSION_MINOR {0}\n'.format(dk[1])
 header+='#define UNICODE_VERSION_PATCH {0}\n'.format(dk[2])
 if fi.optimize==es.em:
  header+='#define UNICORN_OPTIMIZE_FOR_SPEED // cppcheck-suppress misra-c2012-2.5\n'
 else:
  header+='#define UNICORN_OPTIMIZE_FOR_SIZE\n'
 if fi.algorithms.dV&ee.br:
  fa.add(co)
 if fi.algorithms.dV&ee.bs:
  fa.add(at)
 if fi.algorithms.dV&ee.bt:
  fa.add(au)
 if fi.eX&eZ.bM:
  fa.add(W)
 if fi.eX&eZ.bC:
  fa.add(Y)
 if fi.eX&eZ.bK:
  fa.add(Z)
 if fi.eX&eZ.bU:
  fa.add(aa)
 if fi.eX&eZ.bI:
  fa.add(ab)
 if fi.eX&eZ.bV:
  fa.add(ac)
 if fi.eX&eZ.bL:
  fa.add(ad)
 if fi.eX&eZ.bE:
  fa.add(ae)
 if fi.eX&eZ.bF:
  fa.add(af)
 if fi.eX&eZ.bG:
  fa.add(ag)
 if fi.eX&eZ.bJ:
  fa.add(ah)
 if fi.eX&eZ.bO:
  fa.add(ai)
 if fi.eX&eZ.bT:
  fa.add(aj)
 if fi.eX&eZ.bS:
  fa.add(ak)
 if fi.algorithms.compression:
  fa.add(aM)
 if fi.algorithms.segmentation&eg.bz:
  fa.add(aY)
 if fi.algorithms.segmentation&eg.bA:
  fa.add(cp)
 if fi.algorithms.segmentation&eg.bB:
  fa.add(ba)
 if fi.algorithms.normalization&ed.cO:
  fa.add(aL)
 if fi.algorithms.normalization&ed.cP:
  fa.add(dt)
 if fi.algorithms.bW:
  if fi.algorithms.normalization&ed.cO:
   fa.add(aC)
  if fi.algorithms.normalization&ed.cP:
   fa.add(aE)
 if fi.algorithms.collation:
  fa.add(aV)
 if fi.algorithms.do&dH.bu:
  fa.add(cq)
 if fi.algorithms.do&dH.bv:
  fa.add(az)
 if fi.eX&eZ.bH:
  fa.add(am)
 if fi.eX&eZ.bD:
  fa.add(dN)
 if fi.eX&eZ.bN:
  fa.add(ao)
 if fi.eX&eZ.bP:
  fa.add(ci)
 if fi.eX&eZ.bQ:
  fa.add(ck)
 if fi.eX&eZ.bR:
  fa.add(cm)
 if fi.dW&ef.bw:
  fa.add(cr)
 if fi.dW&ef.bx:
  fa.add(bb)
 if fi.dW&ef.by:
  fa.add(bc)
 while True:
  dB=fa.copy()
  for dl in fa:
   dB=dB.union(dl.eP())
  if fa==dB:
   break
  fa=dB
 bd=sorted(list(fa),key=lambda er:er.__class__.__name__)
 fj=fd()
 size=0
 dC=[]
 for dl in bd:
  eI=dl()
  eI.eQ(fj)
  dC.append(eI)
 for eI in dC:
  fh=eI.eY(fg,fj,fi)
  source+=fh.source+'\n'
  header+=fh.header+'\n'
  fe+=fh.fe
  size+=fh.size
  eI.dr+=fh.size
 for eI in dC:
  fh=eI.cU(fg,fj,fi)
  source+=fh.source+'\n'
  header+=fh.header+'\n'
  fe+=fh.fe
  size+=fh.size
  eI.dr+=fh.size
 fh=cM(fj,'CodepointData','get_codepoint_data')
 source+=fh.source
 header+=fh.header
 size+=fh.size
 if args.verbose:
  for eI in dC:
   if eI.dr>0:
    print('Added: {} ({:.2f} kB)'.format(eI.__class__.__name__,eI.dr/1024))
   else:
    print('Added: {}'.format(eI.__class__.__name__))
  print('Character table size: ({:.2f} kB)'.format(fh.size/1024))
  print('Total size: {:.2f} kB'.format(size/1024))
 source=dj(source)
 header=dj(header)
 fe=dj(fe)
 source+='\n'
 header+='\n'
 fe+='\n'
 return (header,source,fe)
def bk(args):
 if not os.path.exists('unicorn.bin'):
  print('error: cannot find unicorn.bin')
  return 1
 fg=zipfile.ZipFile('unicorn.bin','r')
 bf,bg,bh=be(args,fg)
 file=fg.open('code.c')
 bi=file.read().decode('utf-8-sig')
 file.close()
 file=fg.open('code.h')
 bj=file.read().decode('utf-8-sig')
 file.close()
 copyright='/*\n *  Unicorn: Embeddable Unicode algorithms software library.\n *  Copyright (c) 2025 Railgun Labs, LLC\n *\n *  This file is part of Unicorn, distributed under a non-commercial use software\n *  license. For the full terms see the included LICENSE file. If you did not\n *  receive a LICENSE file or you would like to purchase a commercial license,\n *  contact us at <https://RailgunLabs.com/contact/>.\n */\n\n// The Unicorn source code has been amalgamated into a monolithic source\n// and header file. As part of the amalgamation process all code comments\n// and extraneous white space have been removed.\n//\n// The amalgamation is intended for consumption, not development. The\n// unamalgamated source code can be obtained by purchasing a license\n// from Railgun Labs at https://RailgunLabs.com/unicorn/license.\n\n'
 filename=os.path.join(args.output_dir,'unicorn.c')
 fp=open(filename,'w',encoding='utf-8',newline='\n')
 fp.write(copyright)
 fp.write(bf)
 fp.write(bi)
 fp.write('#if !defined(__cppcheck__)\n')
 fp.write(bg)
 fp.write('#endif\n')
 fp.close()
 print('writing:',os.path.realpath(filename))
 filename=os.path.join(args.output_dir,'unicorn.h')
 fp=open(filename,'w',encoding='utf-8',newline='\n')
 fp.write(copyright)
 fp.write('#ifndef UNICORN_H\n')
 fp.write('#define UNICORN_H\n')
 fp.write(bh)
 fp.write(bj)
 fp.write('#endif\n')
 fp.close()
 print('writing:',os.path.realpath(filename))
 return 0
if __name__=='__main__':
 if isinstance(sys.version_info,tuple):
  cG=sys.version_info[0]
  cH=sys.version_info[1]
 else:
  cG=sys.version_info.major
  cH=sys.version_info.minor
 if cG<3 or cH<10:
  raise Exception('This script requires Python 3.10 or newer')
 dD=argparse.ArgumentParser(description='Build Unicorn source and header.')
 dD.add_argument('--config',dest='config_file',action='store',help='path to configuration file')
 dD.add_argument('--output',dest='output_dir',action='store',help='path to write generated C source files')
 dD.add_argument('--verbose',dest='verbose',action='store_true',help='report added features and their size contributions')
 args=di()
 dD.parse_args(namespace=args)
 sys.exit(bk(args))