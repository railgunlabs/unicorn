#!/usr/bin/env python3

#  Unicorn - Embeddable Unicode Library
#  Copyright (c) 2021-2025 Railgun Labs, LLC - All Rights Reserved
#
#  All information contained herein is and remains the property of Railgun Labs.
#  The intellectual and technical concepts contained herein are proprietary to
#  Railgun Labs and may be covered by U.S. and Foreign Patents, patents in process,
#  and are protected by trade secret or copyright law.
#
#  This Python script is an amalgamation of multiple scripts. It is intended for
#  consumption, not development. As part of the amalgamation process the code
#  has been obfuscated. This includes the removal of comments and type hints.
#  You may not reverse engineer, decompile, disassemble, or otherwise attempt
#  to derive the source code or underlying structure of this script.
#
#  The original source code can be obtained by purchasing a license
#  from Railgun Labs at https://RailgunLabs.com/unicorn/license.

b='16.0.0'
from enum import IntEnum
from dataclasses import dataclass
class eU(IntEnum):
 bl=32
 ei=16
 eH=8
 def c(self):
  return int(self/8)
@dataclass
class fd:
 def __init__(self,source='',header='',fc='',size=0):
  self.source=source
  self.header=header
  self.fc=fc
  self.size=size
from typing import List,Dict,Any,Optional
from dataclasses import dataclass
eQ=int
a=1114112
dD=128
@dataclass
class dE:
 name:str
 dP:eQ
 d:eQ
 def __hash__(self):
  return hash(self.name)
 def __eq__(self,dQ):
  if isinstance(dQ,dE):
   return self.name==dQ.name
  return False
def cI(e):
 return [int(fa,16) for fa in e.split()]
class dY:
 def __init__(self,f):
  self.eA=[0]*f
 def __hash__(self):
  bm=5381
  for value in self.eA:
   bm^=hash(value)
  return bm
 def __eq__(self,dQ):
  if isinstance(dQ,dY):
   if len(self.eA)!=len(dQ.eA):
    return False
   for dR in range(len(self.eA)):
    if self.eA[dR]!=dQ.eA[dR]:
     return False
   return True
  return False
class bn:
 def __init__(self,index,name,default_value,dZ):
  self.id=index
  self.g=name
  self.default_value=default_value
  self.dZ=dZ
class fb:
 def __init__(self):
  self.characters={}
  self.eI={}
  self.dm=None
 def eT(self,property,default_value,dZ):
  if property not in self.eI:
   self.eI[property]=bn(len(self.eI),property,default_value,dZ)
  return self.eI[property].id
 def get(self,property):
  return self.eI[property].id
 def h(self):
  self.dm=dY(len(self.eI))
  for eB in self.eI.values():
   self.dm.eA[eB.id]=eB.default_value
 def bo(self,fa):
  if fa in self.characters:
   return self.characters[fa]
  else:
   en=dY(len(self.eI))
   for eB in self.eI.values():
    en.eA[eB.id]=eB.default_value
   self.characters[fa]=en
   return en
 def set(self,fa,property,value):
  self.bo(fa).eA[property]=value
 def ea(self,fa,property,value):
  self.bo(fa).eA[property]|=value
def cM(fh,struct,function):
 if len(fh.eI)==0:
  return fd()
 fh.h()
 ej={}
 assert fh.dm is not None
 ej[fh.dm]=0
 dn=0
 for eR,eu in fh.characters.items():
  if eu not in ej:
   ej[eu]=len(ej)
  if ej[eu]!=0:
   dn=max(dn,eR)
 cJ=[]
 cK=[]
 cL=[]
 eI=sorted(fh.eI.values(),key=lambda eo:eo.dZ,reverse=True)
 for code in range(dn+1):
  if code%dD!=0:
   continue
  dS=[]
  for bp in range(code,code+dD):
   if bp in fh.characters:
    eu=fh.characters[bp]
    if eu in ej:
     dS+=[ej[eu]]
     continue
   dS+=[0]
  if dS in cL:
   cJ+=[cL.index(dS)*dD]
  else:
   cJ+=[len(cK)]
   cK+=dS
   cL+=[dS]
 size=0
 source=''
 source+=f'const struct {struct} *{function}(unichar cp)\n'
 source+='{\n'
 source+=f'    static const struct {struct} {function}_unicode_codepoints[] = {{\n'
 for en in ej.keys():
  source+='        {'
  for eB in eI:
   source+='{0}u,'.format(en.eA[eB.id])
   size+=eB.dZ.c()
  source+='},\n'
 source+='    };\n\n'
 source+=f'    static const uint16_t {function}_stage1_table[] = {{'
 for index,value in enumerate(cJ):
  if index%8==0:
   source+='\n'
   source+='        '
  source+='%du, '%value
  size+=2
 source+='\n'
 source+='    };\n\n'
 source+=f'    static const uint16_t {function}_stage2_table[] = {{'
 for index,value in enumerate(cK):
  if index%8==0:
   source+='\n'
   source+='        '
  source+='%du, '%value
  size+=2
 source+='\n'
 source+='    };\n\n'
 source+=f'    const struct {struct} *data = NULL;\n'
 source+='    if (cp > UNICHAR_C({0}))\n'.format(dn)
 source+='    {\n'
 source+='        data = &{0}_unicode_codepoints[0]; // code point out of range\n'.format(function)
 source+='    }\n'
 source+='    else\n'
 source+='    {\n'
 source+='        const uint16_t stage2_offset = {0}_stage1_table[cp >> UNICHAR_C({1})];\n'.format(function,dD.bit_length()-1)
 source+='        const uint16_t codepoint_index = {0}_stage2_table[stage2_offset + (cp & UNICHAR_C({1}))];\n'.format(function,dD-1)
 source+='        data = &{0}_unicode_codepoints[codepoint_index];\n'.format(function)
 source+='    }\n'
 source+='\n'
 source+='    return data;\n'
 source+='}\n\n'
 i={eU.eH:'uint8_t',eU.ei:'uint16_t',eU.bl:'uint32_t'}
 header=''
 header+=f'struct {struct} {{\n'
 for eB in eI:
  header+=f'     {i[eB.dZ]} {eB.g};\n'
 header+='};\n'
 header+=f'const struct {struct} *{function}(unichar cp);\n'
 return fd(source,header,'',size)
from typing import List,Set,Dict,Any,Optional,Tuple,cast
import sys
import enum
import json
import zipfile
import fnmatch
class dF(enum.Enum):
 bq=enum.auto()
 cN=enum.auto()
class ep(enum.Enum):
 ek=enum.auto()
 j=enum.auto()
class eb(enum.IntFlag):
 eC=enum.auto()
 cO=enum.auto()
 cP=enum.auto()
class ec(enum.IntFlag):
 eC=enum.auto()
 br=enum.auto()
 bs=enum.auto()
 bt=enum.auto()
class dG(enum.IntFlag):
 eC=enum.auto()
 bu=enum.auto()
 bv=enum.auto()
class ed(enum.IntFlag):
 eC=enum.auto()
 bw=enum.auto()
 bx=enum.auto()
 by=enum.auto()
class ee(enum.IntFlag):
 eC=enum.auto()
 bz=enum.auto()
 bA=enum.auto()
 bB=enum.auto()
class eX(enum.IntFlag):
 eC=enum.auto()
 bC=enum.auto()
 bD=enum.auto()
 bE=enum.auto()
 bF=enum.auto()
 bG=enum.auto()
 bH=enum.auto()
 bI=enum.auto()
 bJ=enum.auto()
 bK=enum.auto()
 bL=enum.auto()
 bM=enum.auto()
 bN=enum.auto()
 bO=enum.auto()
 bP=enum.auto()
 bQ=enum.auto()
 bR=enum.auto()
 bS=enum.auto()
 bT=enum.auto()
 bU=enum.auto()
 bV=enum.auto()
class k:
 def __init__(self):
  self.normalization=eb.eC
  self.dT=ec.eC
  self.do=dG.eC
  self.segmentation=ee.eC
  self.compression=False
  self.collation=False
  self.bW=False
class eZ:
 def __init__(self):
  self.endian=dF.cN if sys.byteorder=='big' else dF.bq
  self.bX=True
  self.cQ='uint32_t'
  self.optimize=ep.ek
  self.bY=32
  self.dp=[]
  self.dU=ed.eC
  self.algorithms=k()
  self.eV=eX.eC
 def l(self,fa):
  blocks=self.dp
  cR=0
  cS=len(blocks)-1
  while cR<=cS:
   dq=(cS+cR)//2
   if fa<blocks[dq].dP:
    cS=dq-1
   elif fa>blocks[dq].d:
    cR=dq+1
   else:
    return False
  return True
 def m(self):
  if self.cQ.find('64')>0:
   return 8
  return 4
def q(p):
 dH=[]
 for o in p.split('.'):
  dH.append(int(o))
 while len(dH)<2:
  dH.append(0)
 return (dH[0],dH[1])
def eP(bZ,ca):
 bZ.lower().lstrip('is').replace('_','').replace(' ','')
 ca.lower().rstrip('is').replace('_','').replace(' ','')
 return bZ==ca
def t(fg,algorithms):
 if not isinstance(algorithms,Dict):
  print("error: expected object for 'algorithms'")
  sys.exit(1)
 for key,value in algorithms.items():
  if key=='normalization':
   if not isinstance(value,List):
    print("error: expected string list for 'normalization'")
    sys.exit(1)
   for ev in value:
    if not isinstance(ev,str):
     print("error: expected string list for 'normalization'")
     sys.exit(1)
    if ev.lower()=='nfc':
     fg.algorithms.normalization|=eb.cO
    elif ev.lower()=='nfd':
     fg.algorithms.normalization|=eb.cP
    else:
     print('warning: ignoring unknown normalization form: {0}'.format(ev))
  elif key=='normalizationQuickCheck':
   if not isinstance(value,bool):
    print("error: expected boolean value for 'normalizationQuickCheck'")
    sys.exit(1)
   fg.algorithms.bW=value
  elif key=='caseConversion':
   if not isinstance(value,List):
    print("error: expected string list for 'caseConversion'")
    sys.exit(1)
   for ew in value:
    if not isinstance(ew,str):
     print("error: expected string list for 'caseConversion'")
     sys.exit(1)
    if ew.lower()=='lower':
     fg.algorithms.dT|=ec.br
    elif ew.lower()=='upper':
     fg.algorithms.dT|=ec.bs
    elif ew.lower()=='title':
     fg.algorithms.dT|=ec.bt
    else:
     print('warning: ignoring unknown case conversion target: {0}'.format(ew))
  elif key=='caseFolding':
   if not isinstance(value,List):
    print("error: expected string list for 'caseFolding'")
    sys.exit(1)
   for ew in value:
    if not isinstance(ew,str):
     print("error: expected string list for 'caseFolding'")
     sys.exit(1)
    if ew.lower()=='default':
     fg.algorithms.do|=dG.bu
    elif ew.lower()=='canonical':
     fg.algorithms.do|=dG.bv
    else:
     print('warning: ignoring unknown case fold target: {0}'.format(ew))
  elif key=='segmentation':
   if not isinstance(value,List):
    print("error: expected string list for 'segmentation'")
    sys.exit(1)
   for ev in value:
    if not isinstance(ev,str):
     print("error: expected string list for 'segmentation'")
     sys.exit(1)
    if ev.lower()=='grapheme':
     fg.algorithms.segmentation|=ee.bz
    elif ev.lower()=='word':
     fg.algorithms.segmentation|=ee.bA
    elif ev.lower()=='sentence':
     fg.algorithms.segmentation|=ee.bB
    else:
     print('warning: ignoring unknown segmentation form: {0}'.format(ev))
  elif key=='compression':
   if not isinstance(value,bool):
    print("error: expected boolean value for 'compression'")
    sys.exit(1)
   fg.algorithms.compression=value
  elif key=='collation':
   if not isinstance(value,bool):
    print("error: expected boolean value for 'collation'")
    sys.exit(1)
   fg.algorithms.collation=value
  else:
   print('warning: ignoring unknown algorithm: {0}'.format(key))
def parse(filename,fe):
 fg=eZ()
 if filename is None:
  return fg
 try:
  cb=open(filename,'r',encoding='utf-8')
 except FileNotFoundError:
  print('error: file not found: {0}'.format(filename))
  sys.exit(1)
 ff=json.load(cb)
 cb.close()
 if 'version' not in ff:
  print('error: missing version: {0}'.format(filename))
  sys.exit(1)
 version=q(ff['version'])
 if version[0]<1:
  print('error: illegal version: {0}'.format(version))
  sys.exit(1)
 cc=set()
 file=fe.open('blocks.json')
 blocks=[dE(*block) for block in json.loads(file.read())['blocks']]
 file.close()
 for key,value in ff.items():
  if key=='version':
   continue
  elif key=='endian':
   if not isinstance(value,str):
    print("error: expected string value for 'endian'")
    sys.exit(1)
   if value.lower()=='little':
    fg.endian=dF.bq
   elif value.lower()=='big':
    fg.endian=dF.cN
   elif value.lower()=='native':
    pass
   else:
    print("warning: expected 'little' or 'big' for 'endian'")
  elif key=='optimizeFor':
   if not isinstance(value,str):
    print("error: expected string value for 'optimizeFor'")
    sys.exit(1)
   if value.lower()=='speed':
    fg.optimize=ep.ek
   elif value.lower()=='size':
    fg.optimize=ep.j
   else:
    print("warning: expected 'speed' or 'size' for 'optimizeFor'")
  elif key=='hasStandardAllocators':
   if not isinstance(value,bool):
    print("error: expected boolean value for 'hasStandardAllocators'")
    sys.exit(1)
   fg.bX=value
  elif key=='characterStorage':
   if not isinstance(value,str) or len(value.strip())==0:
    print("error: expected string value for 'characterStorage'")
    sys.exit(1)
   fg.cQ=value
  elif key=='stackBufferSize':
   if not isinstance(value,int) or value<1:
    print("error: expected a positive integer value for 'stackBufferSize'")
    sys.exit(1)
   cT=4
   if value<cT:
    print(f"warning: rounding up 'stackBufferSize' from {value} to {cT} (the minimum)")
    value=cT
   fg.bY=value
  elif key=='excludeCharacterBlocks':
   if not isinstance(value,List):
    print("error: expected string list for 'excludeCharacterBlocks'")
    sys.exit(1)
   for cd in value:
    if not isinstance(cd,str):
     print("error: expected string list for 'excludeCharacterBlocks'")
     sys.exit(1)
    for block in blocks:
     if fnmatch.fnmatch(block.name,cd):
      cc.add(block)
  elif key=='encodingForms':
   if not isinstance(value,List):
    print("error: expected string list for 'encodingForms'")
    sys.exit(1)
   for encoding in value:
    if not isinstance(encoding,str):
     print("error: expected string list for 'encodingForms'")
     sys.exit(1)
    if encoding.lower()=='utf-8':
     fg.dU|=ed.bw
    elif encoding.lower()=='utf-16':
     fg.dU|=ed.bx
    elif encoding.lower()=='utf-32':
     fg.dU|=ed.by
    else:
     print("warning: expected 'utf-8' or 'utf-16' or 'utf-32' for 'encodingForms'")
  elif key=='characterProperties':
   if not isinstance(value,List):
    print("error: expected string list for 'characterProperties'")
    sys.exit(1)
   for eS in value:
    if not isinstance(eS,str):
     print("error: expected string list for 'characterProperties'")
     sys.exit(1)
    if eP(eS,'Alphabetic'):
     fg.eV|=eX.bC
    elif eP(eS,'Canonical_Combining_Class'):
     fg.eV|=eX.bD
    elif eP(eS,'Dash'):
     fg.eV|=eX.bE
    elif eP(eS,'Diacritic'):
     fg.eV|=eX.bF
    elif eP(eS,'Extender'):
     fg.eV|=eX.bG
    elif eP(eS,'General_Category'):
     fg.eV|=eX.bH
    elif eP(eS,'Hex_Digit'):
     fg.eV|=eX.bI
    elif eP(eS,'Ideographic'):
     fg.eV|=eX.bJ
    elif eP(eS,'Lowercase'):
     fg.eV|=eX.bK
    elif eP(eS,'Math'):
     fg.eV|=eX.bL
    elif eP(eS,'Noncharacter_Code_Point'):
     fg.eV|=eX.bM
    elif eP(eS,'Numeric_Value'):
     fg.eV|=eX.bN
    elif eP(eS,'Quotation_Mark'):
     fg.eV|=eX.bO
    elif eP(eS,'Simple_Lowercase_Mapping'):
     fg.eV|=eX.bP
    elif eP(eS,'Simple_Uppercase_Mapping'):
     fg.eV|=eX.bQ
    elif eP(eS,'Simple_Titlecase_Mapping'):
     fg.eV|=eX.bR
    elif eP(eS,'Terminal_Punctuation'):
     fg.eV|=eX.bS
    elif eP(eS,'Unified_Ideograph'):
     fg.eV|=eX.bT
    elif eP(eS,'Uppercase'):
     fg.eV|=eX.bU
    elif eP(eS,'White_Space'):
     fg.eV|=eX.bV
    else:
     print('warning: ignoring unknown character property: {0}'.format(eS))
  elif key=='algorithms':
   t(fg,value)
  else:
   print('warning: ignoring unknown feature: {0}'.format(key))
 for block in cc:
  fg.dp.append(block)
 fg.dp=sorted(fg.dp,key=lambda eo:eo.dP)
 return fg
from typing import Set,Tuple,Type
import zipfile
class Feature(object):
 def __init__(self):
  self.dr=0
 def eO(self,fh):
  pass
 def eW(self,fe,fh,fg):
  return fd()
 def cU(self,fe,fh,fg):
  return fd()
 @staticmethod
 def eN():
  return set()
dV=('flags',0,eU.eH)
ce=1
u=2
v=4
cf=8
cg=16
from typing import Dict,Set,Type
import zipfile
import json
x=1
y=2
z=4
B=8
C=16
D=32
E=64
F=128
G=256
H=512
J=1024
K=2048
N=4096
O=8192
class P(Feature):
 def eO(self,fh):
  fh.eT('binary_properties',0,eU.ei)
 def eW(self,fe,fh,fg):
  fc='#define UNICORN_FEATURE_BINARY_PROPERTIES\n'
  return fd(fc=fc)
class eK(Feature):
 @staticmethod
 def eN():
  return set([P])
 def eJ(self,fe,fh,Q,R,V):
  file=fe.open('binary_properties.json')
  ff=json.loads(file.read())
  mappings=ff[Q]
  file.close()
  binary_properties=fh.get('binary_properties')
  for fa in mappings.keys():
   fh.ea(int(fa,16),binary_properties,R)
  header='#define {0}\n'.format(V)
  return fd(fc=header)
class W(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isNoncharacterCodePoint',x,'UNICORN_FEATURE_NONCHARACTER_CODE_POINT')
class Y(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isAlphabetic',y,'UNICORN_FEATURE_ALPHABETIC')
class Z(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isLowercase',z,'UNICORN_FEATURE_LOWERCASE')
class aa(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isUppercase',B,'UNICORN_FEATURE_UPPERCASE')
class ab(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isHexDigit',C,'UNICORN_FEATURE_HEX_DIGIT')
class ac(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isWhiteSpace',D,'UNICORN_FEATURE_WHITE_SPACE')
class ad(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isMath',E,'UNICORN_FEATURE_MATH')
class ae(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isDash',F,'UNICORN_FEATURE_DASH')
class af(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isDiacritic',G,'UNICORN_FEATURE_DIACRITIC')
class ag(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isExtender',H,'UNICORN_FEATURE_EXTENDER')
class ah(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isIdeographic',J,'UNICORN_FEATURE_IDEOGRAPHIC')
class ai(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isQuotationMark',K,'UNICORN_FEATURE_QUOTATION_MARK')
class aj(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isUnifiedIdeograph',N,'UNICORN_FEATURE_UNIFIED_IDEOGRAPH')
class ak(eK):
 def eW(self,fe,fh,fg):
  return self.eJ(fe,fh,'isTerminalPunctuation',O,'UNICORN_FEATURE_TERMINAL_PUNCTUATION')
from typing import Dict
import zipfile
import json
class am(Feature):
 def eO(self,fh):
  fh.eT('general_category',29,eU.eH)
 def eW(self,fe,fh,fg):
  file=fe.open('gc.json')
  ff=json.loads(file.read())
  characters=ff['characters']
  file.close()
  al=fh.get('general_category')
  for fa,value in characters.items():
   fh.set(int(fa,16),al,value)
  fc='#define UNICORN_FEATURE_GC\n'
  return fd(fc=fc)
from typing import Dict
import zipfile
import json
class ao(Feature):
 def eO(self,fh):
  fh.eT('numeric_value_offset',0,eU.eH)
 def eW(self,fe,fh,fg):
  file=fe.open('numeric_values.json')
  ff=json.loads(file.read())
  mappings=ff['mappings']
  source=ff['source']
  header=ff['header']
  size=ff['size']
  file.close()
  an=fh.get('numeric_value_offset')
  for fa,index in mappings.items():
   fh.set(int(fa,16),an,index)
  fc='#define UNICORN_FEATURE_NUMERIC_VALUE\n'
  return fd(source,header,fc,size)
from typing import List,Dict,Set,Type
import zipfile
import json
er={}
es=[0]
eF=fb()
dI=32768
ex=8191
class ci(Feature):
 @staticmethod
 def eN():
  return set([cV])
 def eO(self,_):
  (eF.eT('simple_lowercase_mapping',ex,eU.ei),)
 def eW(self,fe,_,ef):
  file=fe.open('simple_case_mappings.json')
  ff=json.loads(file.read())
  ap=ff['lowercase']
  file.close()
  ch=eF.get('simple_lowercase_mapping')
  for fa,value in ap.items():
   eR=int(fa,16)
   dJ=int(value,16)
   el=eR-dJ
   if abs(el)>ex:
    if dJ not in er:
     er[dJ]=len(es)
     es.append(dJ)
    eF.set(eR,ch,dI|er[dJ])
   else:
    eF.set(eR,ch,el+ex)
  fc='#define UNICORN_FEATURE_SIMPLE_LOWERCASE_MAPPINGS\n'
  return fd(fc=fc)
class ck(Feature):
 @staticmethod
 def eN():
  return set([cV])
 def eO(self,_):
  (eF.eT('simple_uppercase_mapping',ex,eU.ei),)
 def eW(self,fe,_,ef):
  file=fe.open('simple_case_mappings.json')
  ff=json.loads(file.read())
  aq=ff['uppercase']
  file.close()
  cj=eF.get('simple_uppercase_mapping')
  for fa,value in aq.items():
   eR=int(fa,16)
   dK=int(value,16)
   el=eR-dK
   if abs(el)>ex:
    if dK not in er:
     er[dK]=len(es)
     es.append(dK)
    eF.set(eR,cj,dI|er[dK])
   else:
    eF.set(eR,cj,el+ex)
  fc='#define UNICORN_FEATURE_SIMPLE_UPPERCASE_MAPPINGS\n'
  return fd(fc=fc)
class cm(Feature):
 @staticmethod
 def eN():
  return set([cV])
 def eO(self,_):
  (eF.eT('simple_titlecase_mapping',ex,eU.ei),)
 def eW(self,fe,_,ef):
  file=fe.open('simple_case_mappings.json')
  ff=json.loads(file.read())
  ar=ff['titlecase']
  file.close()
  cl=eF.get('simple_titlecase_mapping')
  for fa,value in ar.items():
   eR=int(fa,16)
   dL=int(value,16)
   el=eR-dL
   if abs(el)>ex:
    if dL not in er:
     er[dL]=len(es)
     es.append(dL)
    eF.set(eR,cl,dI|er[dL])
   else:
    eF.set(eR,cl,el+ex)
  fc='#define UNICORN_FEATURE_SIMPLE_TITLECASE_MAPPINGS\n'
  return fd(fc=fc)
class cV(Feature):
 def cU(self,fe,ef,ds):
  size=0
  source='const unichar unicorn_simple_case_mappings[] = {'
  for index,eR in enumerate(es):
   if index%4==0:
    source+='\n    '
   source+='UNICHAR_C(0x{:04X}), '.format(eR)
   size+=4
  source+='};\n\n'
  header=f'#define CASING_IN_TABLE(C) (((C) & (uint16_t){dI}) == (uint16_t){dI})\n'
  header+='#define GET_CASED_VALUE(C) ((C) & (uint16_t)0x3FFF)\n'
  header+=f'#define CASING_DIFF {ex}\n'
  header+='extern const unichar unicorn_simple_case_mappings[{0}];\n'.format(len(es))
  assert len(es)<65535,'cannot use unsigned 16-bit integer anymore'
  ff=cM(eF,'CharCaseData','unicorn_get_character_casing')
  source+=ff.source
  header+=ff.header
  size+=ff.size
  return fd(source,header,'',size)
from typing import Dict,Set,Type
import zipfile
import json
class cn(Feature):
 def eO(self,fh):
  fh.eT(*dV)
 def eW(self,fe,fh,fg):
  file=fe.open('special_case_mappings.json')
  ff=json.loads(file.read())
  source=ff['source']
  header=ff['header']
  size=ff['size']
  mappings=ff['characterFlags']
  file.close()
  dW=fh.get(dV[0])
  header+='#define IS_CASED {0}u\n'.format(u)
  header+='#define IS_CASE_IGNORABLE {0}u\n'.format(v)
  for fa,flags in mappings.items():
   fh.ea(int(fa,16),dW,flags)
  return fd(source=source,header=header,size=size)
class co(Feature):
 @staticmethod
 def eN():
  return set([ci,cn])
 def eW(self,_,ef,ds):
  fc='#define UNICORN_FEATURE_LOWERCASE_CONVERT\n'
  return fd(fc=fc)
class at(Feature):
 @staticmethod
 def eN():
  return set([ck,cn])
 def eW(self,_,ef,ds):
  fc='#define UNICORN_FEATURE_UPPERCASE_CONVERT\n'
  return fd(fc=fc)
class au(Feature):
 @staticmethod
 def eN():
  return set([cm,cp,dM,co])
 def eW(self,_,ef,ds):
  fc='#define UNICORN_FEATURE_TITLECASE_CONVERT\n'
  return fd(fc=fc)
from typing import List,Set,Type,Tuple
import zipfile
import json
class cq(Feature):
 def eO(self,fh):
  fh.eT('full_casefold_mapping_offset',0,eU.ei)
  fh.eT(*dV)
 def eW(self,fe,fh,fg):
  file=fe.open('casefold.json')
  ff=json.loads(file.read())
  source=ff['source']
  header=ff['header']
  size=ff['size']
  av=ff['hasGreek']
  aw=ff['caseFoldings']
  ax=ff['changesWhenCasefolded']
  file.close()
  dW=fh.get(dV[0])
  ay=fh.get('full_casefold_mapping_offset')
  for fa in av:
   fh.ea(fa,dW,cf)
  for fa in ax:
   fh.ea(fa,dW,cg)
  for fa,offset in aw:
   fh.set(fa,ay,offset)
  header+='#define UNICORN_CHAR_NEEDS_NORMALIZATION {0}u\n'.format(cf)
  header+='#define UNICORN_CHAR_CHANGES_WHEN_CASEFOLDED {0}u\n'.format(cg)
  fc='#define UNICORN_FEATURE_CASEFOLD_DEFAULT\n'
  return fd(source,header,fc,size)
class az(Feature):
 @staticmethod
 def eN():
  return set([cq,dt])
 def eW(self,fe,fh,fg):
  fc='#define UNICORN_FEATURE_CASEFOLD_CANONICAL\n'
  return fd(fc=fc)
from typing import Dict
import zipfile
import json
class dM(Feature):
 def eO(self,fh):
  fh.eT('canonical_combining_class',0,eU.eH)
 def eW(self,fe,fh,fg):
  file=fe.open('ccc.json')
  mappings=json.loads(file.read())['ccc']
  file.close()
  aA=fh.get('canonical_combining_class')
  for fa,ccc in mappings.items():
   fh.set(int(fa,16),aA,ccc)
  fc='#define UNICORN_FEATURE_CCC\n'
  return fd(fc=fc)
from typing import Dict,Set,Type
import zipfile
import json
class aC(Feature):
 @staticmethod
 def eN():
  return set([dM])
 def eO(self,fh):
  fh.eT('quick_check_flags',0,eU.eH)
 def eW(self,fe,fh,_):
  file=fe.open('quickcheck.json')
  ff=json.loads(file.read())
  aB=ff['quickCheckNFC']
  file.close()
  cW=fh.get('quick_check_flags')
  for fa,cX in aB.items():
   fh.ea(int(fa,16),cW,cX<<4)
  fc='#define UNICORN_FEATURE_NFC_QUICK_CHECK\n'
  return fd(fc=fc)
class aE(Feature):
 @staticmethod
 def eN():
  return set([dM])
 def eO(self,fh):
  fh.eT('quick_check_flags',0,eU.eH)
 def eW(self,fe,fh,_):
  file=fe.open('quickcheck.json')
  ff=json.loads(file.read())
  aD=ff['quickCheckNFD']
  file.close()
  cW=fh.get('quick_check_flags')
  for fa,cX in aD.items():
   fh.ea(int(fa,16),cW,cX)
  fc='#define UNICORN_FEATURE_NFD_QUICK_CHECK\n'
  return fd(fc=fc)
from typing import List,Dict,Set,Type
import zipfile
import json
class dt(Feature):
 @staticmethod
 def eN():
  return set([dM,cr])
 def eO(self,fh):
  fh.eT('canonical_decomposition_mapping_offset',0,eU.ei)
 def eW(self,fe,fh,fg):
  aF=fh.get('canonical_decomposition_mapping_offset')
  file=fe.open('normalize.json')
  mappings=json.loads(file.read())['decompositions']
  file.close()
  cY={}
  eD=[0]
  decompositions={}
  for key,value in mappings.items():
   fa=int(key,16)
   mapping=cI(value)
   decompositions[fa]=mapping
   cZ=''.join([chr(aG) for aG in mapping])
   if cZ in cY:
    canonical_decomposition_mapping_offset=cY[cZ]
   else:
    canonical_decomposition_mapping_offset=len(eD)
    cY[cZ]=len(eD)
    if fg.optimize==ep.ek:
     eD.append(len(mapping))
     eD+=mapping
    else:
     eg=[]
     for en in mapping:
      eg+=[int(da) for da in chr(en).encode('utf8')]
     assert len(eg)<255
     eD.append(len(eg))
     eD+=eg
   fh.set(fa,aF,canonical_decomposition_mapping_offset)
  size=len(eD)
  if fg.optimize==ep.ek:
   size*=fg.m()
  du=0
  dv=0
  for fa,cs in decompositions.items():
   for fa in cs:
    dw=[fa]
    dN=0
    while dN<len(dw):
     ct=dw[dN]
     if ct not in decompositions:
      dN+=1
      continue
     dw[dN:dN+1]=decompositions[ct]
    eg=[]
    for en in cs:
     eg+=[int(da) for da in chr(en).encode('utf8')]
    dv=max(dv,len(dw))
    if fg.optimize==ep.ek:
     du=dv
    else:
     du=max(du,len(eg))
  if fg.optimize==ep.ek:
   source='const unichar uni_canonical_decomp_mappings[] = {'
   for index,eR in enumerate(eD):
    if index%6==0:
     source+='\n'
     source+='    '
    source+='UNICHAR_C(0x{:02X}), '.format(eR)
   source+='\n'
   source+='};\n\n'
  else:
   source='const uint8_t uni_canonical_decomp_mappings[] = {'
   for index,eR in enumerate(eD):
    if index%8==0:
     source+='\n'
     source+='    '
    source+='0x{:02X}u, '.format(eR)
   source+='\n'
   source+='};\n\n'
  if fg.optimize==ep.ek:
   header='extern const unichar uni_canonical_decomp_mappings[{0}];\n'.format(len(eD))
  else:
   header='extern const uint8_t uni_canonical_decomp_mappings[{0}];\n'.format(len(eD))
  header+='#define LONGEST_UNICHAR_DECOMPOSITION {0}\n'.format(dv+1)
  header+='#define LONGEST_RAW_DECOMPOSITION {0}\n'.format(du+1)
  fc='#define UNICORN_FEATURE_NFD\n'
  return fd(source,header,fc,size)
from typing import List,Set,Tuple,Type
import zipfile
import json
class aL(Feature):
 @staticmethod
 def eN():
  return set([dt])
 def eO(self,fh):
  fh.eT('canonical_composition_mapping_offset',0,eU.ei)
  fh.eT('canonical_composition_mapping_count',0,eU.eH)
  fh.eT(*dV)
 def eW(self,fe,fh,fg):
  file=fe.open('normalize.json')
  ff=json.loads(file.read())
  mappings=ff['compositionMappings']
  cu=ff['compositionPairs']
  file.close()
  aH=fh.get('canonical_composition_mapping_offset')
  aI=fh.get('canonical_composition_mapping_count')
  dW=fh.get(dV[0])
  for db in mappings:
   dP=db[0]
   offset=db[1]
   count=db[2]
   fh.set(dP,aH,offset)
   fh.set(dP,aI,count)
   fh.ea(dP,dW,ce)
  size=0
  source='const struct CanonicalCompositionPair uni_canonical_comp_pairs[] = {\n'
  for cv in cu:
   aJ=cv[0]
   aK=cv[1]
   source+='    {{ UNICHAR_C(0x{:04X}), UNICHAR_C(0x{:04X}) }},\n'.format(aJ,aK)
   size+=8
  source+='};\n\n'
  header='struct CanonicalCompositionPair\n'
  header+='{\n'
  header+='    unichar codepoint;\n'
  header+='    unichar composed_codepoint;\n'
  header+='};\n'
  header+='#define CHAR_IS_COMPOSABLE {0}u\n'.format(ce)
  header+='extern const struct CanonicalCompositionPair uni_canonical_comp_pairs[{0}];\n'.format(len(cu))
  header+='\n'
  fc='#define UNICORN_FEATURE_NFC\n'
  return fd(source,header,fc,size)
import zipfile
class aM(Feature):
 def eW(self,fe,fh,fg):
  fc='#define UNICORN_FEATURE_COMPRESSION\n'
  return fd(fc=fc)
from typing import List,Dict,Set,Type
import zipfile
import json
aN=1<<31
dc=aN
cw=1<<30
cx=1<<29
cy=dc
class CollationNode:
 def __init__(self):
  self.eu=0
  self.eE={}
  self.cz=0
  self.dX=0
  self.cA=0
 def cB(self,fa):
  if fa in self.eE:
   return self.eE[fa]
  eM=CollationNode()
  eM.eu=fa
  self.eE[fa]=eM
  return eM
 def cC(self,ey):
  eE=list(self.eE.values())
  eE=sorted(eE,key=lambda eo:eo.eu)
  if len(eE)>0:
   self.cz=len(ey)
   for dx in eE:
    dx.cA=len(ey)
    ey.append(dx)
   for dx in eE:
    dx.cC(ey)
 def aO(self):
  ey=[]
  self.cC(ey)
  return ey
class aV(Feature):
 @staticmethod
 def eN():
  return set([dt])
 def eO(self,fh):
  fh.eT('collation_subtype',0,eU.eH)
 def eW(self,fe,fh,fg):
  dO=[0]
  dy={}
  def cD(et):
   nonlocal dO
   nonlocal dy
   dz=[]
   for dR,em in enumerate(et):
    if dR<len(et)-1:
     dz.append(em|cy)
    else:
     dz.append(em)
   dd=' '.join([str(em) for em in dz])
   if dd in dy:
    return dy[dd]
   else:
    dX=len(dO)
    for em in dz:
     dO.append(em)
    dy[dd]=dX
    return dX
  collation=fb()
  de=collation.eT('value',0,eU.bl)
  file=fe.open('allkeys.json')
  ff=json.loads(file.read())
  aP=ff['header']
  mappings=ff['mappings']
  cE=ff['contractionStarters']
  aQ=ff['longestInitialSubstring']
  subtypes=ff['subtypes']
  file.close()
  aR=fh.get('collation_subtype')
  for key,aS in subtypes.items():
   eR=int(key,16)
   fh.set(eR,aR,aS)
  root=CollationNode()
  for string,et in mappings.items():
   dA=string.split()[0]
   if not fg.l(int(dA,16)):
    continue
   if dA in cE:
    eh=cI(string)
    eM=root
    for fa in eh:
     eM=eM.cB(fa)
    assert eM.dX==0,'expected contractions to be unqiue'
    eM.dX=cD(et)
  ey=root.aO()
  df=0
  for string,et in mappings.items():
   df=max(len(et),df)
   eh=cI(string)
   dA=string.split()[0]
   if dA in cE:
    eM=root.cB(eh[0])
    collation.set(eh[0],de,eM.cA|cx)
   else:
    assert len(eh)==1,'expected a single character mapping'
    if len(et)==1:
     collation.set(eh[0],de,et[0]|dc)
    else:
     collation.set(eh[0],de,cD(et)|cw)
  ff=cM(collation,'CollationChararacterData','uni_get_collation_data')
  size=ff.size
  source=ff.source
  source+='const uint32_t unicorn_collation_mappings[] = {\n'
  for dR,em in enumerate(dO):
   if dR%4==0:
    source+='\n    '
   source+='0x{:08X}u, '.format(em)
   size+=4
  source+='};\n\n'
  source+='const struct CollationNode unicorn_collation_mappings_trie[] = {\n'
  for eM in ey:
   assert len(eM.eE)<=255
   eo=len(eM.eE)<<24|eM.eu
   aT=eM.dX
   aU=eM.cz
   source+=f'    {{ {eo}u, {aT}, {aU} }},\n'
   size+=8
  source+='};\n\n'
  header=ff.header
  header+=aP
  header+='#define UNICORN_MAX_COLLATION {0}\n'.format(df+1)
  header+='#define INLINE_CE_FLAG {0}u\n'.format(dc)
  header+='#define IN_WEIGHT_TABLE_FLAG {0}u\n'.format(cw)
  header+='#define IN_TRIE_FLAG {0}u\n'.format(cx)
  header+='#define CONTINUATION_FLAG {0}u\n'.format(cy)
  header+='#define LONGEST_INITIAL_SUBSTRING {0}\n'.format(aQ)
  header+='#define UNICORN_IN_COLLATION_TABLE(NODE) ((NODE)->collation_mappings_offset > (uint16_t)0)\n'
  header+='#define GET_NODE_CODE_POINT(NODE) ((NODE)->codepoint_and_childcount & 0x1FFFFFu)\n'
  header+='#define GET_NODE_CHILD_COUNT(NODE) ((NODE)->codepoint_and_childcount >> 24u)\n'
  header+='struct CollationNode {\n'
  header+='    uint32_t codepoint_and_childcount;\n'
  header+='    uint16_t collation_mappings_offset;\n'
  header+='    uint16_t child_offset;\n'
  header+='};\n'
  header+='extern const struct CollationNode unicorn_collation_mappings_trie[{0}];\n'.format(len(ey))
  header+='extern const uint32_t unicorn_collation_mappings[{0}];\n'.format(len(dO))
  fc='#define UNICORN_FEATURE_COLLATION\n'
  return fd(source,header,fc,size)
from typing import Set,Type
import zipfile
import json
ez=0
class dg(Feature):
 def cU(self,fe,ef,ds):
  gcb=fe.open('segmentation.json')
  ff=json.loads(gcb.read())
  header=ff['header']
  gcb.close()
  header+='#define MAX_BREAK_STATES {0}\n'.format(ez)
  fc='#define UNICORN_FEATURE_SEGMENTATION\n'
  return fd(header=header,fc=fc)
class aY(Feature):
 @staticmethod
 def eN():
  return set([dg])
 def eO(self,fh):
  fh.eT('gcb',0,eU.eH)
  fh.eT('incb',0,eU.eH)
 def eW(self,fe,fh,_):
  gcb=fe.open('gcb.json')
  ff=json.loads(gcb.read())
  header=ff['header']
  source=ff['source']
  size=ff['size']
  gcb.close()
  global ez
  ez=max(int(ff['states']),ez)
  aW=fh.get('gcb')
  for eL in ff['gcb']:
   fh.set(eL[0],aW,eL[1])
  aX=fh.get('incb')
  for eL in ff['incb']:
   fh.set(eL[0],aX,eL[1])
  fc='#define UNICORN_FEATURE_GCB\n'
  return fd(source,header,fc,size)
class cp(Feature):
 @staticmethod
 def eN():
  return set([dg])
 def eO(self,fh):
  fh.eT('wb',0,eU.eH)
  fh.eT('wbx',0,eU.eH)
 def eW(self,fe,fh,_):
  wb=fe.open('wb.json')
  ff=json.loads(wb.read())
  header=ff['header']
  source=ff['source']
  size=ff['size']
  wb.close()
  global ez
  ez=max(int(ff['states']),ez)
  dh=fh.get('wb')
  for eL in ff['wb']:
   fh.set(eL[0],dh,eL[1])
  aZ=fh.get('wbx')
  for eL in ff['wbx']:
   fh.set(eL[0],aZ,eL[1])
  fc='#define UNICORN_FEATURE_WB\n'
  return fd(source,header,fc,size)
class ba(Feature):
 @staticmethod
 def eN():
  return set([dg])
 def eO(self,fh):
  fh.eT('sb',0,eU.eH)
 def eW(self,fe,fh,_):
  wb=fe.open('sb.json')
  ff=json.loads(wb.read())
  header=ff['header']
  source=ff['source']
  size=ff['size']
  wb.close()
  global ez
  ez=max(int(ff['states']),ez)
  dh=fh.get('sb')
  for eL in ff['sb']:
   fh.set(eL[0],dh,eL[1])
  fc='#define UNICORN_FEATURE_SB\n'
  return fd(source,header,fc,size)
import zipfile
class cr(Feature):
 def eW(self,fe,fh,fg):
  fc='#define UNICORN_FEATURE_ENCODING_UTF8\n'
  return fd(fc=fc)
class bb(Feature):
 def eW(self,fe,fh,fg):
  fc='#define UNICORN_FEATURE_ENCODING_UTF16\n'
  return fd(fc=fc)
class bc(Feature):
 def eW(self,fe,fh,fg):
  fc='#define UNICORN_FEATURE_ENCODING_UTF32\n'
  return fd(fc=fc)
from typing import List,Set,Tuple,Type
import sys
import os
import argparse
import zipfile
class di(argparse.Namespace):
 def __init__(self):
  self.config_file='features.json'
  self.output_dir=''
  self.verbose=False
def dj(s):
 return '\n'.join((cF for cF in s.splitlines() if cF.strip()))
def be(args,fe):
 header=''
 source=''
 fc=''
 eY=set()
 fg=parse(args.config_file,fe)
 fc+='#include <stdint.h>\n'
 if fg.endian==dF.cN:
  fc+='#define UNICORN_BIG_ENDIAN\n'
 else:
  fc+='#define UNICORN_LITTLE_ENDIAN\n'
 fc+='#define UNICORN_STACK_BUFFER_SIZE {0}\n'.format(fg.bY)
 fc+='typedef {0} unichar;\n'.format(fg.cQ)
 if fg.bX:
  fc+='#define UNICORN_HAVE_C_MEMORY_ROUTINES\n'
 header+='#include "unicorn.h"\n'
 dk=b.split('.')
 header+='#define UNICODE_VERSION_MAJOR {0}\n'.format(dk[0])
 header+='#define UNICODE_VERSION_MINOR {0}\n'.format(dk[1])
 header+='#define UNICODE_VERSION_PATCH {0}\n'.format(dk[2])
 if fg.optimize==ep.ek:
  header+='#define UNICORN_OPTIMIZE_FOR_SPEED // cppcheck-suppress misra-c2012-2.5\n'
 else:
  header+='#define UNICORN_OPTIMIZE_FOR_SIZE\n'
 if fg.algorithms.dT&ec.br:
  eY.add(co)
 if fg.algorithms.dT&ec.bs:
  eY.add(at)
 if fg.algorithms.dT&ec.bt:
  eY.add(au)
 if fg.eV&eX.bM:
  eY.add(W)
 if fg.eV&eX.bC:
  eY.add(Y)
 if fg.eV&eX.bK:
  eY.add(Z)
 if fg.eV&eX.bU:
  eY.add(aa)
 if fg.eV&eX.bI:
  eY.add(ab)
 if fg.eV&eX.bV:
  eY.add(ac)
 if fg.eV&eX.bL:
  eY.add(ad)
 if fg.eV&eX.bE:
  eY.add(ae)
 if fg.eV&eX.bF:
  eY.add(af)
 if fg.eV&eX.bG:
  eY.add(ag)
 if fg.eV&eX.bJ:
  eY.add(ah)
 if fg.eV&eX.bO:
  eY.add(ai)
 if fg.eV&eX.bT:
  eY.add(aj)
 if fg.eV&eX.bS:
  eY.add(ak)
 if fg.algorithms.compression:
  eY.add(aM)
 if fg.algorithms.segmentation&ee.bz:
  eY.add(aY)
 if fg.algorithms.segmentation&ee.bA:
  eY.add(cp)
 if fg.algorithms.segmentation&ee.bB:
  eY.add(ba)
 if fg.algorithms.normalization&eb.cO:
  eY.add(aL)
 if fg.algorithms.normalization&eb.cP:
  eY.add(dt)
 if fg.algorithms.bW:
  if fg.algorithms.normalization&eb.cO:
   eY.add(aC)
  if fg.algorithms.normalization&eb.cP:
   eY.add(aE)
 if fg.algorithms.collation:
  eY.add(aV)
 if fg.algorithms.do&dG.bu:
  eY.add(cq)
 if fg.algorithms.do&dG.bv:
  eY.add(az)
 if fg.eV&eX.bH:
  eY.add(am)
 if fg.eV&eX.bD:
  eY.add(dM)
 if fg.eV&eX.bN:
  eY.add(ao)
 if fg.eV&eX.bP:
  eY.add(ci)
 if fg.eV&eX.bQ:
  eY.add(ck)
 if fg.eV&eX.bR:
  eY.add(cm)
 if fg.dU&ed.bw:
  eY.add(cr)
 if fg.dU&ed.bx:
  eY.add(bb)
 if fg.dU&ed.by:
  eY.add(bc)
 while True:
  dB=eY.copy()
  for dl in eY:
   dB=dB.union(dl.eN())
  if eY==dB:
   break
  eY=dB
 bd=sorted(list(eY),key=lambda eo:eo.__class__.__name__)
 fh=fb()
 size=0
 dC=[]
 for dl in bd:
  eG=dl()
  eG.eO(fh)
  dC.append(eG)
 for eG in dC:
  ff=eG.eW(fe,fh,fg)
  source+=ff.source+'\n'
  header+=ff.header+'\n'
  fc+=ff.fc
  size+=ff.size
  eG.dr+=ff.size
 for eG in dC:
  ff=eG.cU(fe,fh,fg)
  source+=ff.source+'\n'
  header+=ff.header+'\n'
  fc+=ff.fc
  size+=ff.size
  eG.dr+=ff.size
 ff=cM(fh,'CodepointData','get_codepoint_data')
 source+=ff.source
 header+=ff.header
 size+=ff.size
 if args.verbose:
  for eG in dC:
   if eG.dr>0:
    print('Added: {} ({:.2f} kB)'.format(eG.__class__.__name__,eG.dr/1024))
   else:
    print('Added: {}'.format(eG.__class__.__name__))
  print('Character table size: ({:.2f} kB)'.format(ff.size/1024))
  print('Total size: {:.2f} kB'.format(size/1024))
 source=dj(source)
 header=dj(header)
 fc=dj(fc)
 source+='\n'
 header+='\n'
 fc+='\n'
 return (header,source,fc)
def bk(args):
 if not os.path.exists('unicorn.bin'):
  print('error: cannot find unicorn.bin')
  return 1
 fe=zipfile.ZipFile('unicorn.bin','r')
 bf,bg,bh=be(args,fe)
 file=fe.open('code.c')
 bi=file.read().decode('utf-8-sig')
 file.close()
 file=fe.open('code.h')
 bj=file.read().decode('utf-8-sig')
 file.close()
 copyright='/*\n *  Unicorn: Embeddable Unicode algorithms software library.\n *  Copyright (c) 2025 Railgun Labs, LLC\n *\n *  This file is part of Unicorn, distributed under a non-commercial use software\n *  license. For the full terms see the included LICENSE file. If you did not\n *  receive a LICENSE file or you would like to purchase a commercial license,\n *  contact us at <https://RailgunLabs.com/contact/>.\n */\n\n// The Unicorn source code has been amalgamated into a monolithic source\n// and header file. As part of the amalgamation process all code comments\n// and extraneous white space have been removed.\n//\n// The amalgamation is intended for consumption, not development. The\n// unamalgamated source code can be obtained by purchasing a license\n// from Railgun Labs at https://RailgunLabs.com/unicorn/license.\n\n'
 filename=os.path.join(args.output_dir,'unicorn.c')
 fp=open(filename,'w',encoding='utf-8',newline='\n')
 fp.write(copyright)
 fp.write(bf)
 fp.write(bi)
 fp.write('#if !defined(__cppcheck__)\n')
 fp.write(bg)
 fp.write('#endif\n')
 fp.close()
 print('writing:',os.path.realpath(filename))
 filename=os.path.join(args.output_dir,'unicorn.h')
 fp=open(filename,'w',encoding='utf-8',newline='\n')
 fp.write(copyright)
 fp.write('#ifndef UNICORN_H\n')
 fp.write('#define UNICORN_H\n')
 fp.write(bh)
 fp.write(bj)
 fp.write('#endif\n')
 fp.close()
 print('writing:',os.path.realpath(filename))
 return 0
if __name__=='__main__':
 if isinstance(sys.version_info,tuple):
  cG=sys.version_info[0]
  cH=sys.version_info[1]
 else:
  cG=sys.version_info.major
  cH=sys.version_info.minor
 if cG<3 or cH<10:
  raise Exception('This script requires Python 3.10 or newer')
 parser=argparse.ArgumentParser(description='Build Unicorn source and header.')
 parser.add_argument('--config',dest='config_file',action='store',help='path to configuration file')
 parser.add_argument('--output',dest='output_dir',action='store',help='path to write generated C source files')
 parser.add_argument('--verbose',dest='verbose',action='store_true',help='report added features and their size contributions')
 args=di()
 parser.parse_args(namespace=args)
 sys.exit(bk(args))